{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd823329a4543ea96cf0d7b3684fd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model for DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "model = TFAutoModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "def compute_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**encoded_input)\n",
    "    embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Load a subset of the wikipedia dataset (assuming structure and availability)\n",
    "dataset = load_dataset(\"Cohere/wikipedia-22-12-en-embeddings\",split=\"train\", streaming=True)\n",
    "d0 = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#========Exercise 3.1 =========== \n",
    "# Fill in the following code\n",
    "# ===============================\n",
    "\n",
    "def make_dataset(dataset,max_num_of_articles=None):\n",
    "    data_list = []\n",
    "    if max_num_of_articles:\n",
    "        # print(f\"using {max_num_of_articles} articles\")\n",
    "        for i, example in enumerate(dataset):\n",
    "            if i >= max_num_of_articles:\n",
    "                break\n",
    "            data_list.append(example)\n",
    "        print(f\"using {i} articles\")\n",
    "\n",
    "        # Convert to DataFrame\n",
    "    else: # get all the data fronm the iterator \n",
    "        print(\"using all articles\")\n",
    "        for example in tqdm(dataset):\n",
    "            data_list.append(example)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "def find_most_relevant_article(query_embedding, df,max_num_of_articles=None):\n",
    "    # df = make_dataset(dataset,max_num_of_articles)\n",
    "    max_similarity = -1\n",
    "\n",
    "    most_relevant_article = ''\n",
    "    \n",
    "    query_embedding = np.array(query_embedding[0][:])\n",
    "    query_embedding = query_embedding.reshape(1,-1)\n",
    "    # print(query_embedding.shape)\n",
    "    for row in range(len(df)):\n",
    "        # article_embedding = row[:]\n",
    "\n",
    "        # article_embedding = np.array(article_embedding[1]['emb'])\n",
    "        article_embedding = df.iloc[row]['emb']\n",
    "        article_embedding = np.array(article_embedding).reshape(1,-1)\n",
    "\n",
    "        # article_embedding = np.array(article_embedding)\n",
    "\n",
    "        # print('using the following article:\\n', row[1]['text'])\n",
    "        # print(article_embedding.shape)\n",
    "\n",
    "        similarity_vec = cosine_similarity(query_embedding, article_embedding)\n",
    "        # print(similarity_vec)\n",
    "        if similarity_vec > max_similarity:\n",
    "            max_similarity = similarity_vec\n",
    "            most_relevant_article = df.iloc[row]['text']\n",
    "        # i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return most_relevant_article, max_similarity\n",
    "\n",
    "# def make_dataset_and_find_most_relevant_article(query_embedding, dataset,max_num_of_articles=None):\n",
    "#     df = make_dataset(dataset,max_num_of_articles)\n",
    "#     most_relevant_article, similarity_amount = find_most_relevant_article(query_embedding, df)\n",
    "#     return most_relevant_article, similarity_amount\n",
    "\n",
    "NUM = 20000\n",
    "# df = make_dataset(dataset,max_num_of_articles=NUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 007ff12f-c119-4916-8852-c2348ec0c5fb)')' thrown while requesting GET https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings/resolve/85c2eca83d4b9dcecc043c23748cb8c1047f683f/data/train-00000-of-00253-8d3dffb4e6ef0304.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5f0b46c7-7932-4d5a-8fc7-4798436c0999)')' thrown while requesting GET https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings/resolve/85c2eca83d4b9dcecc043c23748cb8c1047f683f/data/train-00000-of-00253-8d3dffb4e6ef0304.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 20000 articles\n"
     ]
    }
   ],
   "source": [
    "df = make_dataset(dataset,NUM)\n",
    "# Example input string\n",
    "input_text = \"Elon Musk\"\n",
    "\n",
    "# # Compute the embedding for the input text\n",
    "input_embedding = compute_embedding(input_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Article:   Unlike previous \"Batman\" films, \"The Batman\" focuses on Batman's detective\n",
      "skills, with Reeves describing it as an \"almost-noir driven, detective version of Batman\"\n",
      "emphasizing the character's heart and mind. He said the film blended the detective, action, horror,\n",
      "and psychological thriller genres, which he felt hewed closer to the comics than previous\n",
      "adaptations had. He also felt this approach made it the most frightening \"Batman\" film. Reeves\n",
      "looked to films and filmmakers from the New Hollywood era for inspiration, including \"The French\n",
      "Connection\" (1971), \"Klute\" (1971), \"Chinatown\" (1974), \"All the President's Men\" (1976), and \"Taxi\n",
      "Driver\" (1976), as well as the works of Alfred Hitchcock and Wong Kar-wai's short film \"The Hand\"\n",
      "(2004). \"Chinatown\" and \"All the President's Men\" influenced \"The Batman\" depiction of a corrupt,\n",
      "decaying Gotham, while the relationship between Donald Sutherland and Jane Fonda's characters in\n",
      "\"Klute\" inspired the dynamic between Batman and Catwoman. Reeves said that he blended the\n",
      "inspirations to \"inform the story, motivation, imagery, and tone\" while also \"conjur[ing]\n",
      "something... evocative and unique\". To convey Batman's insecurity, Reeves added a scene, inspired by\n",
      "\"Manhunter\" (1986), featuring him visiting the Joker to profile the Riddler. Reeves also intended\n",
      "for the Joker's appearance to signify that Gotham's troubles would not end after the Riddler is\n",
      "captured.\n",
      "Similarity Score: [[0.05421749]]\n"
     ]
    }
   ],
   "source": [
    "# Example input string\n",
    "input_text = \"Elon Musk\"\n",
    "\n",
    "# # Compute the embedding for the input text\n",
    "input_embedding = compute_embedding(input_text)\n",
    "\n",
    "# Find the most relevant article\n",
    "# To reduce the runtime, look at only the first N articles\n",
    "article_idx, similarity = find_most_relevant_article(input_embedding, df,max_num_of_articles=NUM)\n",
    "print(fill(\"Most Relevant Article: \\n \"+article_idx, width=100))\n",
    "print(\"Similarity Score:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embdding of: Leonardo DiCaprio\n",
      "Most Relevant Article:   Sessions for West's sixth solo effort begin to take shape in early 2013 in\n",
      "his own personal loft's living room at a Paris hotel. Determined to \"undermine the commercial\", he\n",
      "once again brought together close collaborators and attempted to incorporate Chicago drill,\n",
      "dancehall, acid house, and industrial music. Primarily inspired by architecture, West's\n",
      "perfectionist tendencies led him to contact producer Rick Rubin fifteen days shy of its due date to\n",
      "strip down the record's sound in favor of a more minimalist approach. Initial promotion of his sixth\n",
      "album included worldwide video projections of the album's music and live television performances.\n",
      "\"Yeezus\", West's sixth album, was released June 18, 2013, to rave reviews from critics. It became\n",
      "his sixth consecutive number one debut, but also marked his lowest solo opening week sales.\n",
      "Similarity Score: [[0.07458331]]\n",
      "\n",
      "\n",
      "Input embdding of: France\n",
      "Most Relevant Article:   There are seven international seaports: Keelung, Taipei, Suao, Taichung,\n",
      "Kaohsiung, Anping, and Hualien. The Port of Kaohsiung handled the largest volume of cargo in Taiwan,\n",
      "with about 440 million shipping tonnes, which accounted for 58.6% of Taiwan's total throughput in\n",
      "2021. The shipping tonnage followed by Taichung (18.6%), Taipei (12%) and Keelung (8.7%).\n",
      "Similarity Score: [[0.0862608]]\n",
      "\n",
      "\n",
      "Input embdding of: Python\n",
      "Most Relevant Article:   Eastwood's next film \"The Eiger Sanction\" (1975) was based on Trevanian's\n",
      "critically acclaimed spy novel of the same name. Eastwood plays Jonathan Hemlock in a role\n",
      "originally intended for Paul Newman, an assassin turned college art professor who decides to return\n",
      "to his former profession for one last \"sanction\" in return for a rare Pissarro painting. In the\n",
      "process he must climb the north face of the Eiger in Switzerland under perilous conditions. Mike\n",
      "Hoover taught Eastwood how to climb during several weeks of preparation at Yosemite in the summer of\n",
      "1974 before filming commenced in Grindelwald, Switzerland on August 12, 1974. Despite prior warnings\n",
      "about the perils of the Eiger, Eastwood insisted on doing all his own climbing and stunts. The film\n",
      "crew suffered a number of accidents, including one fatality. Upon release in May 1975 \"The Eiger\n",
      "Sanction\" was marginally successful commercially, receiving $14.2 million at the box-office, and\n",
      "gained mixed reviews. Joy Gould Boyum of \"The Wall Street Journal\" dismissed the film as \"brutal\n",
      "fantasy\". Eastwood blamed Universal Studios for the film's poor promotion and turned his back on\n",
      "them to make an agreement with Warner Brothers, through Frank Wells, that has lasted to the present\n",
      "day.\n",
      "Similarity Score: [[0.05137892]]\n",
      "\n",
      "\n",
      "Input embdding of: Deep Learning\n",
      "Most Relevant Article:   In 1958, Eastwood was cast as Rowdy Yates for the CBS hour-long western\n",
      "series \"Rawhide\", the career breakthrough he had long sought. Eastwood was not especially happy with\n",
      "his character; Eastwood was almost 30, and Rowdy was too young and cloddish for his comfort. Filming\n",
      "began in Arizona in the summer of 1958. It took just three weeks for \"Rawhide\" to reach the top 20\n",
      "in TV ratings and although it never won an Emmy, it was a major success for several years, and\n",
      "peaked at number six in the ratings between October 1960 and April 1961. The \"Rawhide\" years\n",
      "(1959–65) were some of the most grueling of Eastwood's career, often filming six days a week for an\n",
      "average of 12 hours a day, but some directors still criticized him for not working hard enough. By\n",
      "late 1963, \"Rawhide\" was beginning to decline in the ratings and lacked freshness in the scripts; it\n",
      "was canceled in the middle of the 1965–66 season. Eastwood made his first attempt at directing when\n",
      "he filmed several trailers for the show, but was unable to convince producers to let him direct an\n",
      "episode. In the show's first season, Eastwood earned $750 an episode. At the time of \"Rawhide\"s\n",
      "cancelation, he received $119,000 an episode as severance pay.\n",
      "Similarity Score: [[0.04654783]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = ['Leonardo DiCaprio',\n",
    "         'France',\n",
    "         'Python',\n",
    "         'Deep Learning']\n",
    "\n",
    "for i in input:\n",
    "    input_embedding = compute_embedding(i)\n",
    "    article, similarity =find_most_relevant_article(input_embedding, df,max_num_of_articles=1000)\n",
    "    print(\"Input embdding of:\", i)\n",
    "    print(fill(\"Most Relevant Article: \\n \"+ article,width=100))\n",
    "    print(\"Similarity Score:\", similarity)\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
