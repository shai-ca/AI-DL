{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  []\n",
      "Is Metal enabled: []\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Resizing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def load_dataset(validation_split=0.2,dec_factor=10):\n",
    "    # Load the CIFAR-10 dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "    # Reduce the number of images by a factor of dec_factor\n",
    "    train_images = train_images[::dec_factor]  # Take every Nth image\n",
    "    train_labels = train_labels[::dec_factor]  # Corresponding labels\n",
    "    test_images = test_images[::dec_factor]\n",
    "    test_labels = test_labels[::dec_factor]\n",
    "\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        train_images, train_labels, test_size=validation_split, random_state=42)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    train_images, val_images, test_images = train_images / 255.0, val_images / 255.0, test_images / 255.0\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    val_labels = to_categorical(val_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "    return train_images, train_labels, val_images, val_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    image_size = 128\n",
    "    base_model = MobileNet( input_shape = (image_size,image_size,3),\n",
    "                            include_top=False,\n",
    "                             weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = models.Sequential([        \n",
    "        Resizing(image_size, image_size, interpolation=\"nearest\", input_shape=train_images.shape[1:]),\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Specify the learning rate\n",
    "    \n",
    "    # Instantiate the Adam optimizer with the default learning rate\n",
    "    optimizer = Adam()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_train_vs_val_accuracy(history):\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     t = np.array(history.epoch)\n",
    "#     colors = {}\n",
    "#     colors['loss'] = 'r'\n",
    "#     colors['val_loss'] = '#FEA510'\n",
    "#     colors['accuracy'] = 'green'\n",
    "#     colors['val_accuracy'] = 'b'\n",
    "\n",
    "#     # plt.figure(figsize=(10, 5))\n",
    "#     i = 1\n",
    "#     for elem in history.history.keys():\n",
    "#         # i can check if the string contains a substring for subplot\n",
    "#         if i%2 ==0:\n",
    "#             plt.subplot(1,2,1)\n",
    "#         else:\n",
    "#             plt.subplot(1,2,2)\n",
    "\n",
    "#         y = np.array(history.history[elem])\n",
    "#         c = str(colors[elem])\n",
    "#         l = str(elem)\n",
    "#         plt.plot(np.array(t),np.array(y),color=c,label=l)\n",
    "#         i += 1\n",
    "\n",
    "#         plt.legend()\n",
    "#         # plt.show()\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Amplitude')\n",
    "#     return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "os.environ['PYTHONHASHSEED'] = str(42)  # This variable influences the hash function's behavior in Python 3.3 and later.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Load the dataset\n",
    "train_images, train_labels, val_images, val_labels, test_images, test_labels = load_dataset()\n",
    "#Create the backbone model that will be used to train\n",
    "model = create_model()\n",
    "#Do the actual training\n",
    "history = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))\n",
    "#Evaluate \n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}, Test loss: {test_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_train_vs_val_metrics(history, figsize=(8, 3)):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics (loss and accuracy) from model training history.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : tensorflow.keras.callbacks.History\n",
    "        The training history object returned by model.fit()\n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height), default is (12, 5)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The created figure object containing both plots\n",
    "    \"\"\"\n",
    "    # Define consistent styling\n",
    "    METRICS_CONFIG = {\n",
    "        'loss': {'color': 'red', 'subplot': 0, 'title': 'Model Loss'},\n",
    "        'val_loss': {'color': '#FEA510', 'subplot': 0},\n",
    "        'accuracy': {'color': 'green', 'subplot': 1, 'title': 'Model Accuracy'},\n",
    "        'val_accuracy': {'color': 'blue', 'subplot': 1}\n",
    "    }\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    axes = [ax1, ax2]\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric, values in history.history.items():\n",
    "        if metric in METRICS_CONFIG:\n",
    "            config = METRICS_CONFIG[metric]\n",
    "            subplot_idx = config['subplot']\n",
    "            ax = axes[subplot_idx]\n",
    "            \n",
    "            ax.plot(epochs, values, \n",
    "                   color=config['color'],\n",
    "                   label=metric.replace('_', ' ').title(),\n",
    "                   linewidth=2)\n",
    "            \n",
    "            # Set titles and labels only once per subplot\n",
    "            if 'title' in config:\n",
    "                ax.set_title(config['title'])\n",
    "                ax.set_ylabel(config['title'].split()[1])\n",
    "\n",
    "            ax.set_xlabel('Epochs')\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # return fig\n",
    "plot_train_vs_val_metrics(history, figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a # Unfreeze the layers (base_model.trainable) so that the whole network can learn\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable=True\n",
    "\n",
    "[setattr(layer, 'trainable', True) for layer in model.layers]\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.Sequential(model.layers)  # Copy existing layers\n",
    "new_model.add(tf.keras.layers.Dropout(0.5))    # Add dropoutos.environ['TF_ENABLE_GRAPH_REWRITE'] = '0'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
