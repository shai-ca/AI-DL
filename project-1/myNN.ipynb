{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# Load the data\n",
    "wandb.login()\n",
    "data = pd.read_csv(\"./MIMIC_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "# data_clean = data.dropna()\n",
    "# y = data_clean['outcome']\n",
    "# X = data_clean.drop(columns='outcome')\n",
    "def balance_dataset(df, column, method='undersample', random_state=42):\n",
    "    \"\"\"\n",
    "    Balance a dataset based on the values in a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    column (str): Column name to balance by\n",
    "    method (str): 'undersample' or 'oversample'\n",
    "    random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Balanced DataFrame\n",
    "    \"\"\"\n",
    "    # Get value counts\n",
    "    value_counts = df[column].value_counts()\n",
    "    print(f\"Original class distribution:\\n{value_counts}\\n\")\n",
    "    \n",
    "    if method == 'undersample':\n",
    "        # Get the minimum class count\n",
    "        min_count = value_counts.min()\n",
    "        \n",
    "        # Create balanced dataframe\n",
    "        balanced_df = pd.concat([\n",
    "            resample(df[df[column] == val],\n",
    "                    replace=False,\n",
    "                    n_samples=min_count,\n",
    "                    random_state=random_state)\n",
    "            for val in value_counts.index\n",
    "        ])\n",
    "        \n",
    "    elif method == 'oversample':\n",
    "        # Get the maximum class count\n",
    "        max_count = value_counts.max()\n",
    "        \n",
    "        # Create balanced dataframe\n",
    "        balanced_df = pd.concat([\n",
    "            resample(df[df[column] == val],\n",
    "                    replace=True,\n",
    "                    n_samples=max_count,\n",
    "                    random_state=random_state)\n",
    "            if count < max_count else df[df[column] == val]\n",
    "            for val, count in value_counts.items()\n",
    "        ])\n",
    "    \n",
    "    print(f\"Balanced class distribution:\\n{balanced_df[column].value_counts()}\")\n",
    "    return balanced_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "balanced_data_under = balance_dataset(data, 'outcome', method='undersample', random_state=42)\n",
    "balanced_data_over = balance_dataset(data, 'outcome', method='oversample', random_state=42)\n",
    "\n",
    "print(data.shape)\n",
    "print(balanced_data_under.shape)\n",
    "print(balanced_data_over.shape)\n",
    "\n",
    "data = balanced_data_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# build input pipeline using tf.data\n",
    "\n",
    "\n",
    "def get_train_and_val(data_feaure_and_prediction, BATCH_SIZE = 64):\n",
    "    \"\"\"_summary_\n",
    "    this function gets the relevant feaure and variables \n",
    "    Args:\n",
    "        data_feaure_and_prediction (_type_): the last colum should be the output, the first ones should be the feature(s)\n",
    "        BATCH_SIZE (int, optional): _description_. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # data_feaure_and_prediction = data_feaure_and_prediction.dropna() # drop the irrelevant values\n",
    "    X = data_feaure_and_prediction[data_feaure_and_prediction.keys()[:-1]] # the feature\n",
    "    y = data_feaure_and_prediction[data_feaure_and_prediction.keys()[-1]] # its prediction\n",
    "\n",
    "    X_standard = StandardScaler()\n",
    "    X_standard = X_standard.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    return [train_dataset, val_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(feature_num, input_name=\"input name\"):\n",
    "    inputs = keras.Input((feature_num,), name=input_name)\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x1 = keras.layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    x2 = keras.layers.Dropout(0.3)(x2)\n",
    "\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(y, logits) # update the weights of the network\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def train(train_dataset, val_dataset,  model, optimizer,\n",
    "          train_acc_metric, val_acc_metric, loss_fn,\n",
    "          epochs=10, log_step=200, val_log_step=50):\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        train_loss = []   \n",
    "        val_loss = []\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss_value = train_step(x_batch_train, y_batch_train, \n",
    "                                    model, optimizer, \n",
    "                                    loss_fn, train_acc_metric)\n",
    "            train_loss.append(float(loss_value))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "            val_loss_value = test_step(x_batch_val, y_batch_val, \n",
    "                                       model, loss_fn, \n",
    "                                       val_acc_metric)\n",
    "            val_loss.append(float(val_loss_value))\n",
    "            \n",
    "        # Display metrics at the end of each epoch\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_acc_metric.reset_state()\n",
    "        val_acc_metric.reset_state()\n",
    "\n",
    "        # ⭐: log metrics using wandb.log\n",
    "        wandb.log({'epochs': epoch,\n",
    "                   'loss': np.mean(train_loss),\n",
    "                   'acc': float(train_acc), \n",
    "                   'val_loss': np.mean(val_loss),\n",
    "                   'val_acc':float(val_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize wandb with your project name and optionally with configutations.\n",
    "# # play around with the config values and see the result on your wandb dashboard.\n",
    "# feature = [\"age\"]\n",
    "\n",
    "# config = {\n",
    "#             \"epochs\": 10,\n",
    "#             \"batch_size\": 32,\n",
    "#             \"log_step\": 200,\n",
    "#             \"val_log_step\": 50,\n",
    "#             \"architecture\": \"CNN\",\n",
    "#             \"dataset\": \"MIMIC\"\n",
    "#     }\n",
    "\n",
    "# run = wandb.init(project='my-tf-integration', config=config)\n",
    "# config = wandb.config\n",
    "\n",
    "# prediction = \"outcome\"\n",
    "\n",
    "# model_data = data[feature+[prediction]].dropna()\n",
    "# [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "# # Initialize model.\n",
    "# model = make_model(len(feature))\n",
    "\n",
    "# # Instantiate an optimizer to train the model.\n",
    "# optimizer = keras.optimizers.Adam()\n",
    "# # Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Prepare the metrics.\n",
    "# train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "# train(train_dataset,\n",
    "#     val_dataset, \n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_acc_metric,\n",
    "#     val_acc_metric,\n",
    "#     epochs=config.epochs, \n",
    "#     log_step=config.log_step, \n",
    "#     val_log_step=config.val_log_step)\n",
    "\n",
    "# run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize wandb with your project name and optionally with configutations.\n",
    "# play around with the config values and see the result on your wandb dashboard.\n",
    "\n",
    "def run_with_feature(feature):\n",
    "      config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 32,\n",
    "            \"log_step\": 200,\n",
    "            \"val_log_step\": 50,\n",
    "            \"feature_used\": str(feature)\n",
    "      }\n",
    "\n",
    "      run = wandb.init(project='my-tf-integration', config=config)\n",
    "      config = wandb.config\n",
    "\n",
    "      prediction = \"outcome\"\n",
    "\n",
    "      model_data = data[feature+[prediction]].dropna()\n",
    "      [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "      # Initialize model.\n",
    "      model = make_model(len(feature))\n",
    "\n",
    "      # Instantiate an optimizer to train the model.\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      # Instantiate a loss function.\n",
    "      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "      # Prepare the metrics.\n",
    "      train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "      val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "      train(train_dataset,\n",
    "            val_dataset, \n",
    "            model,\n",
    "            optimizer,\n",
    "            train_acc_metric,\n",
    "            val_acc_metric,\n",
    "            loss_fn,\n",
    "            epochs=config.epochs, \n",
    "            log_step=config.log_step, \n",
    "            val_log_step=config.val_log_step)\n",
    "\n",
    "      run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n",
    "# feature = [\"age\"]\n",
    "# run_with_feature(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = data.drop(columns=['outcome']).keys()\n",
    "len(data_keys)\n",
    "# data_keys = data_keys[ \n",
    "\n",
    "train_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "val_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "\n",
    "for elem in data_keys:\n",
    "    # try:\n",
    "    run_with_feature([elem])\n",
    "    #     train_acc_dict[elem] = train_acc_metric\n",
    "    #     val_acc_dict[elem] = val_acc_dict\n",
    "    # except:\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shaicahana/Documents/AI_DL/colab/AI-DL/project-1/wandb/run-20241026_175758-ur18286l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/ur18286l' target=\"_blank\">fragrant-field-218</a></strong> to <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/ur18286l' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/ur18286l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training acc over epoch: 0.1710\n",
      "Validation acc: 0.4805\n",
      "\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.4756\n",
      "Validation acc: 0.6688\n",
      "\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.6466\n",
      "Validation acc: 0.7468\n",
      "\n",
      "Start of epoch 3\n",
      "Training acc over epoch: 0.6954\n",
      "Validation acc: 0.7662\n",
      "\n",
      "Start of epoch 4\n",
      "Training acc over epoch: 0.7117\n",
      "Validation acc: 0.7727\n",
      "\n",
      "Start of epoch 5\n",
      "Training acc over epoch: 0.7313\n",
      "Validation acc: 0.8052\n",
      "\n",
      "Start of epoch 6\n",
      "Training acc over epoch: 0.7818\n",
      "Validation acc: 0.8377\n",
      "\n",
      "Start of epoch 7\n",
      "Training acc over epoch: 0.7785\n",
      "Validation acc: 0.8312\n",
      "\n",
      "Start of epoch 8\n",
      "Training acc over epoch: 0.7866\n",
      "Validation acc: 0.8506\n",
      "\n",
      "Start of epoch 9\n",
      "Training acc over epoch: 0.8029\n",
      "Validation acc: 0.8636\n",
      "\n",
      "Start of epoch 10\n",
      "Training acc over epoch: 0.8436\n",
      "Validation acc: 0.8766\n",
      "\n",
      "Start of epoch 11\n",
      "Training acc over epoch: 0.8404\n",
      "Validation acc: 0.8831\n",
      "\n",
      "Start of epoch 12\n",
      "Training acc over epoch: 0.8599\n",
      "Validation acc: 0.8896\n",
      "\n",
      "Start of epoch 13\n",
      "Training acc over epoch: 0.8485\n",
      "Validation acc: 0.9026\n",
      "\n",
      "Start of epoch 14\n",
      "Training acc over epoch: 0.8779\n",
      "Validation acc: 0.8961\n",
      "\n",
      "Start of epoch 15\n",
      "Training acc over epoch: 0.8795\n",
      "Validation acc: 0.8896\n",
      "\n",
      "Start of epoch 16\n",
      "Training acc over epoch: 0.8811\n",
      "Validation acc: 0.8896\n",
      "\n",
      "Start of epoch 17\n",
      "Training acc over epoch: 0.8681\n",
      "Validation acc: 0.8896\n",
      "\n",
      "Start of epoch 18\n",
      "Training acc over epoch: 0.8893\n",
      "Validation acc: 0.8831\n",
      "\n",
      "Start of epoch 19\n",
      "Training acc over epoch: 0.9055\n",
      "Validation acc: 0.9026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▆▆▆▆▇▇▇▇▇▇█▇██████</td></tr><tr><td>epochs</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▇▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.90554</td></tr><tr><td>epochs</td><td>19</td></tr><tr><td>loss</td><td>0.24534</td></tr><tr><td>val_acc</td><td>0.9026</td></tr><tr><td>val_loss</td><td>0.25483</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-field-218</strong> at: <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/ur18286l' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/ur18286l</a><br/> View project at: <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241026_175758-ur18286l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# feature0 = ['Urine output', 'Lymphocyte', 'Bicarbonate', 'Leucocyte', 'Urea nitrogen', 'Anion gap', 'Neutrophils', 'Blood calcium', 'Lactic acid','PH']\n",
    "# run_with_feature(feature0)\n",
    "\n",
    "feature1 =[\"Urine output\", \"Lymphocyte\", \"Bicarbonate\", \"Leucocyte\", \"Urea nitrogen\",\n",
    "    \"Anion gap\", \"Neutrophils\", \"Blood calcium\", \"Lactic acid\", \"PH\", \"Basophils\",\n",
    "    \"Respiratory rate\", \"Blood sodium\", \"RDW\", \"Blood potassium\", \"NT-proBNP\", \"PT\",\n",
    "    \"Systolic blood pressure\", \"heart rate\", \"Renal failure\", \"Chloride\", \"Platelets\",\n",
    "    \"INR\", \"atrialfibrillation\", \"deficiencyanemias\", \"Diastolic blood pressure\",\n",
    "    \"hypertensive\", \"EF\", \"BMI\", \"Magnesium ion\", \"temperature\", \"Creatinine\", \"MCH\",\n",
    "    \"PCO2\", \"SP O2\", \"hematocrit\", \"MCV\", \"diabetes\", \"Hyperlipemia\", \"age\", \"ID\",\n",
    "    \"COPD\", \"CHD with no MI\", \"Creatine kinase\", \"glucose\", \"gendera\", \"depression\"\n",
    "]\n",
    "\n",
    "run_with_feature(feature1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for practice sake I will also write this NN with the same normal annotation we learned in class.\n",
    "X = clean_data[['age','Blood sodium']]\n",
    "y = clean_data[['outcome']]\n",
    "\n",
    "# standardtize the data\n",
    "X_standrd = StandardScaler()\n",
    "X_standrd = X_standrd.fit_transform(X)\n",
    "\n",
    "\n",
    "# 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standrd, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3 construct DNN\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu',input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 4 Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1, batch_size=32)\n",
    "\n",
    "# what does this does?\n",
    "eval_ = model.evaluate(X_test, y_test, verbose=1)[1]\n",
    "print(eval_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
