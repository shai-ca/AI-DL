{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# Load the data\n",
    "wandb.login()\n",
    "data = pd.read_csv(\"./MIMIC_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean = data.dropna()\n",
    "# y = data_clean['outcome']\n",
    "# X = data_clean.drop(columns='outcome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# build input pipeline using tf.data\n",
    "\n",
    "\n",
    "def get_train_and_val(data_feaure_and_prediction,BATCH_SIZE = 64):\n",
    "    \"\"\"_summary_\n",
    "    this function gets the relevant feaure and variables \n",
    "    Args:\n",
    "        data_feaure_and_prediction (_type_): the last colum should be the output, the first ones should be the feature(s)\n",
    "        BATCH_SIZE (int, optional): _description_. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # data_feaure_and_prediction = data_feaure_and_prediction.dropna() # drop the irrelevant values\n",
    "    X = data_feaure_and_prediction[data_feaure_and_prediction.keys()[:-1]] # the feature\n",
    "    y = data_feaure_and_prediction[data_feaure_and_prediction.keys()[-1]] # its prediction\n",
    "\n",
    "    X_standard = StandardScaler()\n",
    "    X_standard = X_standard.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    return [train_dataset, val_dataset]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(feature_num, input_name=\"input name\"):\n",
    "    inputs = keras.Input((feature_num,), name=input_name)\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x1 = keras.layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    x2 = keras.layers.Dropout(0.3)(x2)\n",
    "\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(y, logits) # update the weights of the network\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def train(train_dataset, val_dataset,  model, optimizer,\n",
    "          train_acc_metric, val_acc_metric, loss_fn,\n",
    "          epochs=10, log_step=200, val_log_step=50):\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        train_loss = []   \n",
    "        val_loss = []\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss_value = train_step(x_batch_train, y_batch_train, \n",
    "                                    model, optimizer, \n",
    "                                    loss_fn, train_acc_metric)\n",
    "            train_loss.append(float(loss_value))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "            val_loss_value = test_step(x_batch_val, y_batch_val, \n",
    "                                       model, loss_fn, \n",
    "                                       val_acc_metric)\n",
    "            val_loss.append(float(val_loss_value))\n",
    "            \n",
    "        # Display metrics at the end of each epoch\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_acc_metric.reset_state()\n",
    "        val_acc_metric.reset_state()\n",
    "\n",
    "        # ‚≠ê: log metrics using wandb.log\n",
    "        wandb.log({'epochs': epoch,\n",
    "                   'loss': np.mean(train_loss),\n",
    "                   'acc': float(train_acc), \n",
    "                   'val_loss': np.mean(val_loss),\n",
    "                   'val_acc':float(val_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize wandb with your project name and optionally with configutations.\n",
    "# # play around with the config values and see the result on your wandb dashboard.\n",
    "# feature = [\"age\"]\n",
    "\n",
    "# config = {\n",
    "#             \"epochs\": 10,\n",
    "#             \"batch_size\": 32,\n",
    "#             \"log_step\": 200,\n",
    "#             \"val_log_step\": 50,\n",
    "#             \"architecture\": \"CNN\",\n",
    "#             \"dataset\": \"MIMIC\"\n",
    "#     }\n",
    "\n",
    "# run = wandb.init(project='my-tf-integration', config=config)\n",
    "# config = wandb.config\n",
    "\n",
    "# prediction = \"outcome\"\n",
    "\n",
    "# model_data = data[feature+[prediction]].dropna()\n",
    "# [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "# # Initialize model.\n",
    "# model = make_model(len(feature))\n",
    "\n",
    "# # Instantiate an optimizer to train the model.\n",
    "# optimizer = keras.optimizers.Adam()\n",
    "# # Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Prepare the metrics.\n",
    "# train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "# train(train_dataset,\n",
    "#     val_dataset, \n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_acc_metric,\n",
    "#     val_acc_metric,\n",
    "#     epochs=config.epochs, \n",
    "#     log_step=config.log_step, \n",
    "#     val_log_step=config.val_log_step)\n",
    "\n",
    "# run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shaicahana/Documents/AI_DL/colab/AI-DL/project-1/wandb/run-20241026_151934-bddde141</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/bddde141' target=\"_blank\">age</a></strong> to <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/bddde141' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/bddde141</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training acc over epoch: 0.4128\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.8670\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 3\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 4\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 5\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 6\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 7\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 8\n",
      "Training acc over epoch: 0.8745\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 9\n",
      "Training acc over epoch: 0.8723\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 10\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 11\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 12\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 13\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 14\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 15\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 16\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 17\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 18\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 19\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 20\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 21\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 22\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 23\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 24\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 25\n",
      "Training acc over epoch: 0.8723\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 26\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 27\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 28\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 29\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 30\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 31\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 32\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 33\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 34\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 35\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 36\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 37\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 38\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 39\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 40\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 41\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 42\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 43\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 44\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 45\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 46\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 47\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 48\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 49\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 50\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 51\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 52\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 53\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 54\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 55\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 56\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 57\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 58\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 59\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 60\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 61\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 62\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 63\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 64\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 65\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 66\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 67\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 68\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 69\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 70\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 71\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 72\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 73\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 74\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 75\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 76\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 77\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 78\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 79\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 80\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 81\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 82\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 83\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 84\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 85\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 86\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 87\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 88\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 89\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 90\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 91\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 92\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 93\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 94\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 95\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 96\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 97\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 98\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n",
      "\n",
      "Start of epoch 99\n",
      "Training acc over epoch: 0.8734\n",
      "Validation acc: 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m       run\u001b[38;5;241m.\u001b[39mfinish()  \u001b[38;5;66;03m# In Jupyter/Colab, let us know you're finished!\u001b[39;00m\n\u001b[1;32m     50\u001b[0m feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m \u001b[43mrun_with_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 47\u001b[0m, in \u001b[0;36mrun_with_feature\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m     33\u001b[0m val_acc_metric \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()\n\u001b[1;32m     36\u001b[0m train(train_dataset,\n\u001b[1;32m     37\u001b[0m       val_dataset, \n\u001b[1;32m     38\u001b[0m       model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m       log_step\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_step, \n\u001b[1;32m     45\u001b[0m       val_log_step\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mval_log_step)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:452\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2151\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\u001b[38;5;28mself\u001b[39m, exit_code: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     \u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   2143\u001b[0m \n\u001b[1;32m   2144\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2185\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2438\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2435\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2438\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2688\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2681\u001b[0m exit_handle\u001b[38;5;241m.\u001b[39madd_probe(on_probe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_probe_exit)\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m progress\u001b[38;5;241m.\u001b[39mprogress_printer(\n\u001b[1;32m   2684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[1;32m   2685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings,\n\u001b[1;32m   2686\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress_printer:\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;66;03m# Wait for the run to complete.\u001b[39;00m\n\u001b[0;32m-> 2688\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mexit_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_exit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;66;03m# Print some final statistics.\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize wandb with your project name and optionally with configutations.\n",
    "# play around with the config values and see the result on your wandb dashboard.\n",
    "\n",
    "def run_with_feature(feature):\n",
    "      config = {\n",
    "                  \"epochs\": 20,\n",
    "                  \"batch_size\": 32,\n",
    "                  \"log_step\": 200,\n",
    "                  \"val_log_step\": 50,\n",
    "                  \"architecture\": \"CNN\",\n",
    "                  \"dataset\": \"MIMIC\"\n",
    "            }\n",
    "\n",
    "      run = wandb.init(project='my-tf-integration', config=config, name=str(feature[0]))\n",
    "      config = wandb.config\n",
    "\n",
    "      prediction = \"outcome\"\n",
    "\n",
    "      model_data = data[feature+[prediction]].dropna()\n",
    "      [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "      # Initialize model.\n",
    "      model = make_model(len(feature))\n",
    "\n",
    "      # Instantiate an optimizer to train the model.\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      # Instantiate a loss function.\n",
    "      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "      # Prepare the metrics.\n",
    "      train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "      val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "      train(train_dataset,\n",
    "            val_dataset, \n",
    "            model,\n",
    "            optimizer,\n",
    "            train_acc_metric,\n",
    "            val_acc_metric,\n",
    "            loss_fn,\n",
    "            epochs=config.epochs, \n",
    "            log_step=config.log_step, \n",
    "            val_log_step=config.val_log_step)\n",
    "\n",
    "      run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n",
    "# feature = [\"age\"]\n",
    "# run_with_feature(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys = data.drop(columns=['outcome']).keys()\n",
    "len(data_keys)\n",
    "# data_keys = data_keys[ \n",
    "\n",
    "train_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "val_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "\n",
    "for elem in data_keys:\n",
    "    # try:\n",
    "    run_with_feature([elem])\n",
    "    #     train_acc_dict[elem] = train_acc_metric\n",
    "    #     val_acc_dict[elem] = val_acc_dict\n",
    "    # except:\n",
    "    #     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
