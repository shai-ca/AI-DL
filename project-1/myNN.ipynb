{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"./MIMIC_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean = data.dropna()\n",
    "# y = data_clean['outcome']\n",
    "# X = data_clean.drop(columns='outcome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# build input pipeline using tf.data\n",
    "\n",
    "\n",
    "def get_train_and_val(data_feaure_and_prediction,BATCH_SIZE = 64):\n",
    "    \"\"\"_summary_\n",
    "    this function gets the relevant feaure and variables \n",
    "    Args:\n",
    "        data_feaure_and_prediction (_type_): the last colum should be the output, the first ones should be the feature(s)\n",
    "        BATCH_SIZE (int, optional): _description_. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # data_feaure_and_prediction = data_feaure_and_prediction.dropna() # drop the irrelevant values\n",
    "    X = data_feaure_and_prediction[data_feaure_and_prediction.keys()[:-1]] # the feature\n",
    "    y = data_feaure_and_prediction[data_feaure_and_prediction.keys()[-1]] # its prediction\n",
    "\n",
    "    X_standard = StandardScaler()\n",
    "    X_standard = X_standard.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    return [train_dataset, val_dataset]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(feature_num, input_name=\"input name\"):\n",
    "    inputs = keras.Input((feature_num,), name=input_name)\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x1 = keras.layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    x2 = keras.layers.Dropout(0.3)(x2)\n",
    "\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(y, logits) # update the weights of the network\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def train(train_dataset, val_dataset,  model, optimizer,\n",
    "          train_acc_metric, val_acc_metric,\n",
    "          epochs=10,  log_step=200, val_log_step=50):\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        train_loss = []   \n",
    "        val_loss = []\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss_value = train_step(x_batch_train, y_batch_train, \n",
    "                                    model, optimizer, \n",
    "                                    loss_fn, train_acc_metric)\n",
    "            train_loss.append(float(loss_value))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "            val_loss_value = test_step(x_batch_val, y_batch_val, \n",
    "                                       model, loss_fn, \n",
    "                                       val_acc_metric)\n",
    "            val_loss.append(float(val_loss_value))\n",
    "            \n",
    "        # Display metrics at the end of each epoch\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_acc_metric.reset_state()\n",
    "        val_acc_metric.reset_state()\n",
    "\n",
    "        # ‚≠ê: log metrics using wandb.log\n",
    "        wandb.log({'epochs': epoch,\n",
    "                   'loss': np.mean(train_loss),\n",
    "                   'acc': float(train_acc), \n",
    "                   'val_loss': np.mean(val_loss),\n",
    "                   'val_acc':float(val_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shaicahana/Documents/AI_DL/colab/AI-DL/project-1/wandb/run-20241026_142426-37j5kj2l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/37j5kj2l' target=\"_blank\">classic-sea-59</a></strong> to <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/37j5kj2l' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/37j5kj2l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training acc over epoch: 0.0988\n",
      "Validation acc: 0.5959\n",
      "\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.7451\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 3\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 4\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 5\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 6\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 7\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 8\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 9\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 10\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 11\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 12\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 13\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 14\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 15\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 16\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 17\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 18\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 19\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 20\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 21\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 22\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 23\n",
      "Training acc over epoch: 0.8869\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 24\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 25\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 26\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 27\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 28\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 29\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 30\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 31\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 32\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 33\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 34\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 35\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 36\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 37\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 38\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 39\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 40\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 41\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 42\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 43\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 44\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 45\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 46\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 47\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 48\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 49\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 50\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 51\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 52\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 53\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 54\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 55\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 56\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 57\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 58\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 59\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 60\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 61\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 62\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 63\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 64\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 65\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 66\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 67\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 68\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 69\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 70\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 71\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 72\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 73\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 74\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 75\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 76\n",
      "Training acc over epoch: 0.8869\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 77\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 78\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 79\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 80\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 81\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 82\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 83\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 84\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 85\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 86\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 87\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 88\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 89\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 90\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 91\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 92\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 93\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 94\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 95\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 96\n",
      "Training acc over epoch: 0.8869\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 97\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 98\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n",
      "\n",
      "Start of epoch 99\n",
      "Training acc over epoch: 0.8882\n",
      "Validation acc: 0.8549\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>epochs</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.88817</td></tr><tr><td>epochs</td><td>99</td></tr><tr><td>loss</td><td>0.33572</td></tr><tr><td>val_acc</td><td>0.85492</td></tr><tr><td>val_loss</td><td>0.32702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-sea-59</strong> at: <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/37j5kj2l' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration/runs/37j5kj2l</a><br/> View project at: <a href='https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration' target=\"_blank\">https://wandb.ai/shaicahana-hebrew-university-of-jerusalem/my-tf-integration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241026_142426-37j5kj2l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize wandb with your project name and optionally with configutations.\n",
    "# play around with the config values and see the result on your wandb dashboard.\n",
    "\n",
    "config = {\n",
    "              \"epochs\": 100,\n",
    "              \"batch_size\": 32,\n",
    "              \"log_step\": 200,\n",
    "              \"val_log_step\": 50,\n",
    "              \"architecture\": \"CNN\",\n",
    "              \"dataset\": \"MIMIC\"\n",
    "           }\n",
    "\n",
    "run = wandb.init(project='my-tf-integration', config=config)\n",
    "config = wandb.config\n",
    "\n",
    "feature = [\"age\",\"BMI\"]\n",
    "prediction = \"outcome\"\n",
    "# cols = feature + [prediction]   \n",
    "model_data = data[feature+[prediction]].dropna()\n",
    "[train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "# Initialize model.\n",
    "model = make_model(len(feature))\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "train(train_dataset,\n",
    "      val_dataset, \n",
    "      model,\n",
    "      optimizer,\n",
    "      train_acc_metric,\n",
    "      val_acc_metric,\n",
    "      epochs=config.epochs, \n",
    "      log_step=config.log_step, \n",
    "      val_log_step=config.val_log_step)\n",
    "\n",
    "run.finish()  # In Jupyter/Colab, let us know you're finished!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset)\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# x_train = np.reshape(x_train, (-1, 784))\n",
    "# x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# # build input pipeline using tf.data\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "# val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# print(train_dataset)\n",
    "\n",
    "# <_BatchDataset element_spec=(TensorSpec(shape=(None, 2), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>\n",
    "# <_BatchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
