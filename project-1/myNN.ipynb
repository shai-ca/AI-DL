{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "# Load the data\n",
    "wandb.login()\n",
    "data = pd.read_csv(\"./MIMIC_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean = data.dropna()\n",
    "# y = data_clean['outcome']\n",
    "# X = data_clean.drop(columns='outcome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# build input pipeline using tf.data\n",
    "\n",
    "\n",
    "def get_train_and_val(data_feaure_and_prediction,BATCH_SIZE = 64):\n",
    "    \"\"\"_summary_\n",
    "    this function gets the relevant feaure and variables \n",
    "    Args:\n",
    "        data_feaure_and_prediction (_type_): the last colum should be the output, the first ones should be the feature(s)\n",
    "        BATCH_SIZE (int, optional): _description_. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # data_feaure_and_prediction = data_feaure_and_prediction.dropna() # drop the irrelevant values\n",
    "    X = data_feaure_and_prediction[data_feaure_and_prediction.keys()[:-1]] # the feature\n",
    "    y = data_feaure_and_prediction[data_feaure_and_prediction.keys()[-1]] # its prediction\n",
    "\n",
    "    X_standard = StandardScaler()\n",
    "    X_standard = X_standard.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    return [train_dataset, val_dataset]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(feature_num, input_name=\"input name\"):\n",
    "    inputs = keras.Input((feature_num,), name=input_name)\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x1 = keras.layers.Dropout(0.3)(x1)\n",
    "\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    x2 = keras.layers.Dropout(0.3)(x2)\n",
    "\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(y, logits) # update the weights of the network\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def train(train_dataset, val_dataset,  model, optimizer,\n",
    "          train_acc_metric, val_acc_metric, loss_fn,\n",
    "          epochs=10, log_step=200, val_log_step=50):\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        train_loss = []   \n",
    "        val_loss = []\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss_value = train_step(x_batch_train, y_batch_train, \n",
    "                                    model, optimizer, \n",
    "                                    loss_fn, train_acc_metric)\n",
    "            train_loss.append(float(loss_value))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "            val_loss_value = test_step(x_batch_val, y_batch_val, \n",
    "                                       model, loss_fn, \n",
    "                                       val_acc_metric)\n",
    "            val_loss.append(float(val_loss_value))\n",
    "            \n",
    "        # Display metrics at the end of each epoch\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_acc_metric.reset_state()\n",
    "        val_acc_metric.reset_state()\n",
    "\n",
    "        # ⭐: log metrics using wandb.log\n",
    "        wandb.log({'epochs': epoch,\n",
    "                   'loss': np.mean(train_loss),\n",
    "                   'acc': float(train_acc), \n",
    "                   'val_loss': np.mean(val_loss),\n",
    "                   'val_acc':float(val_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize wandb with your project name and optionally with configutations.\n",
    "# # play around with the config values and see the result on your wandb dashboard.\n",
    "# feature = [\"age\"]\n",
    "\n",
    "# config = {\n",
    "#             \"epochs\": 10,\n",
    "#             \"batch_size\": 32,\n",
    "#             \"log_step\": 200,\n",
    "#             \"val_log_step\": 50,\n",
    "#             \"architecture\": \"CNN\",\n",
    "#             \"dataset\": \"MIMIC\"\n",
    "#     }\n",
    "\n",
    "# run = wandb.init(project='my-tf-integration', config=config)\n",
    "# config = wandb.config\n",
    "\n",
    "# prediction = \"outcome\"\n",
    "\n",
    "# model_data = data[feature+[prediction]].dropna()\n",
    "# [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "# # Initialize model.\n",
    "# model = make_model(len(feature))\n",
    "\n",
    "# # Instantiate an optimizer to train the model.\n",
    "# optimizer = keras.optimizers.Adam()\n",
    "# # Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Prepare the metrics.\n",
    "# train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "# train(train_dataset,\n",
    "#     val_dataset, \n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_acc_metric,\n",
    "#     val_acc_metric,\n",
    "#     epochs=config.epochs, \n",
    "#     log_step=config.log_step, \n",
    "#     val_log_step=config.val_log_step)\n",
    "\n",
    "# run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdf77298b1a437d9b33dd1a708ec41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011163694288888514, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize wandb with your project name and optionally with configutations.\n",
    "# play around with the config values and see the result on your wandb dashboard.\n",
    "\n",
    "def run_with_feature(feature):\n",
    "      config = {\n",
    "                  \"epochs\": 10,\n",
    "                  \"batch_size\": 32,\n",
    "                  \"log_step\": 200,\n",
    "                  \"val_log_step\": 50,\n",
    "                  \"architecture\": \"CNN\",\n",
    "                  \"dataset\": \"MIMIC\"\n",
    "            }\n",
    "\n",
    "      run = wandb.init(project='my-tf-integration', config=config, name=str(feature[0]))\n",
    "      config = wandb.config\n",
    "\n",
    "      prediction = \"outcome\"\n",
    "\n",
    "      model_data = data[feature+[prediction]].dropna()\n",
    "      [train_dataset, val_dataset] = get_train_and_val(model_data)\n",
    "\n",
    "\n",
    "      # Initialize model.\n",
    "      model = make_model(len(feature))\n",
    "\n",
    "      # Instantiate an optimizer to train the model.\n",
    "      optimizer = keras.optimizers.Adam()\n",
    "      # Instantiate a loss function.\n",
    "      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "      # Prepare the metrics.\n",
    "      train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "      val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "      train(train_dataset,\n",
    "            val_dataset, \n",
    "            model,\n",
    "            optimizer,\n",
    "            train_acc_metric,\n",
    "            val_acc_metric,\n",
    "            loss_fn,\n",
    "            epochs=config.epochs, \n",
    "            log_step=config.log_step, \n",
    "            val_log_step=config.val_log_step)\n",
    "\n",
    "      run.finish()  # In Jupyter/Colab, let us know you're finished!\n",
    "\n",
    "\n",
    "feature = [\"age\"]\n",
    "run_with_feature(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_keys = data.drop(columns=['outcome']).keys()\n",
    "# data_keys = data_keys[:30] \n",
    "\n",
    "# train_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "# val_acc_dict = dict.fromkeys(data_keys, 0)\n",
    "\n",
    "# for elem in data_keys:\n",
    "#     # try:\n",
    "#     run_with_feature([elem])\n",
    "#     #     train_acc_dict[elem] = train_acc_metric\n",
    "#     #     val_acc_dict[elem] = val_acc_dict\n",
    "#     # except:\n",
    "#     #     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
