{"cells":[{"cell_type":"markdown","metadata":{"id":"ArcUg0sAgZH9"},"source":["# Project #1 Detecting Patterns in Tabular Medical Data with MIMIC-III\n","You can read more about the dataset here: https://www.kaggle.com/datasets/saurabhshahane/in-hospital-mortality-prediction\n","\n","Dr. Barak Or"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMS_Yl44bfT2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWf5Lx5sd4Pe"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEW3x386eGcx"},"outputs":[],"source":["# my_path=\"/content/drive/MyDrive/Courses/Deep Learning Google Reichman Tech School/Project 1 MIMIC/\"\n","my_path=\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"W3xQP2QYEpLe"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5ZnhMyufopj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.svm import SVR\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, classification_report, confusion_matrix\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","\n","# Load the data\n","data = pd.read_csv(\"./MIMIC_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zv-kpCu3fxig"},"outputs":[],"source":["def balance_dataset(df, column, method='undersample', random_state=42):\n","    \"\"\"\n","    Balance a dataset based on the values in a specified column.\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input DataFrame\n","    column (str): Column name to balance by\n","    method (str): 'undersample' or 'oversample'\n","    random_state (int): Random state for reproducibility\n","\n","    Returns:\n","    pandas.DataFrame: Balanced DataFrame\n","    \"\"\"\n","    # Get value counts\n","    value_counts = df[column].value_counts()\n","    print(f\"Original class distribution:\\n{value_counts}\\n\")\n","\n","    if method == 'undersample':\n","        # Get the minimum class count\n","        min_count = value_counts.min()\n","\n","        # Create balanced dataframe\n","        balanced_df = pd.concat([\n","            resample(df[df[column] == val],\n","                    replace=False,\n","                    n_samples=min_count,\n","                    random_state=random_state)\n","            for val in value_counts.index\n","        ])\n","\n","    elif method == 'oversample':\n","        # Get the maximum class count\n","        max_count = value_counts.max()\n","\n","        # Create balanced dataframe\n","        balanced_df = pd.concat([\n","            resample(df[df[column] == val],\n","                    replace=True,\n","                    n_samples=max_count,\n","                    random_state=random_state)\n","            if count < max_count else df[df[column] == val]\n","            for val, count in value_counts.items()\n","        ])\n","\n","    print(f\"Balanced class distribution:\\n{balanced_df[column].value_counts()}\")\n","    return balanced_df.reset_index(drop=True)\n","\n","\n","\n","balanced_data_under = balance_dataset(data, 'outcome', method='undersample', random_state=42)\n","balanced_data_over = balance_dataset(data, 'outcome', method='oversample', random_state=42)\n","\n","print(data.shape)\n","print(balanced_data_under.shape)\n","print(balanced_data_over.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"W_gj2095fYyU"},"source":["# Q1.a\n","What are the mean, median, mode, and standard deviation of the age, BMI, and Blood sodium columns in the dataset? Why are these statistics important for understanding the data?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvh24N6xFRvj"},"outputs":[],"source":["def calc_stats(dat):\n","  return dat.mean(),dat.mode()[0], dat.median(), dat.std()\n","\n","#make it to a df\n","d = {}\n","d['age'] = calc_stats(data['age'])\n","d['BMI'] = calc_stats(data['BMI'])\n","d['Blood sodium'] = calc_stats(data['Blood sodium'])\n","\n","d_ = pd.DataFrame(d)\n","d_['stat']= ['mean','mode','median','std']\n","\n","d_ = d_.set_index('stat')\n","print(d_)\n"]},{"cell_type":"markdown","metadata":{"id":"RbOQ30bybfT7"},"source":["# Answer Q1. a\n","\n","These stats are important because they give us the general first impression of the data.\n","They allow us to understand what are the common values and ranges and develope intuition about the data and if there is something unusal about it."]},{"cell_type":"markdown","metadata":{"id":"y9m2EnZ-hKeB"},"source":["# Q1.b\n","How do the distributions of age, BMI, and Blood sodium look in the dataset? What can we learn from these distributions about the patient population?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aCmbqM6bfT8"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Set the style for the plots\n","sns.set(style='whitegrid')\n","\n","# Histogram for Age Distribution\n","plt.figure(figsize=(10, 6))  # Optional: set figure size\n","sns.histplot(data['age'], bins=20, kde=True)  # kde=True adds a Kernel Density Estimate\n","plt.title(f\"Age Distribution. Mean: {d_['age']['mean']:.2f} Std: {d_['age']['std']:.3f}\")\n","plt.xlabel('Age')\n","plt.ylabel('Count')\n","plt.show()\n","# We have more samples from older patients\n","\n","# Histogram for BMI Distribution\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data['BMI'], bins=20, kde=True)\n","plt.title(f\"BMI Distribution. Mean: {d_['BMI']['mean']:.2f} Std: {d_['BMI']['std']:.2f}\")\n","plt.xlabel('BMI')\n","plt.ylabel('Count')\n","plt.show()\n","# The mean fits to obese, meaning the average health of the patients is not ideal to begin with\n","\n","# Histogram for Blood Sodium Distribution\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data['Blood sodium'], bins=20, kde=True)\n","plt.title(f\"Blood Sodium Distribution. Mean: {d_['Blood sodium']['mean']:.2f} Std: {d_['Blood sodium']['std']:.2f}\")\n","plt.xlabel('Blood Sodium Levels')\n","plt.ylabel('Count')\n","plt.show()\n","# the mean blood sodium levels appear to be in the normal range\n"]},{"cell_type":"markdown","metadata":{"id":"l2Mq9jIVkAbw"},"source":["# Q 1.c\n","Use pandas and scikit-learn to drop rows with missing values in the 'BMI' and 'Blood sodium' columns, and then uses logistic regression, SVM, kNN, and decision tree to predict an 'outcome' based on the features 'age', 'BMI', and 'Blood sodium'. Ensure to split the data using train_test_split with a 20% test size and a random state of 42. Finally, fit the model, make predictions on the test set, and print a report of the best model (“classification_report”). Explain the result of the confusion matrix for the best model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2Yx4wZTgujq"},"outputs":[],"source":["\n","print(data.shape)\n","print(data.dropna(subset = 'BMI' or 'Blood sodium').shape)\n","# data = balanced_data_over\n","data.dropna().head()\n","data = data.dropna(subset = 'BMI' or 'Blood sodium') # the data without nans in BMI or Blood sodium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmD0RuFUkBzR"},"outputs":[],"source":["# split to train and test\n","X = pd.DataFrame(data, columns=['age','BMI','Blood sodium'])\n","y = pd.Series(data['outcome'], name='outcome') #choose one coloum\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoB6xXxkbfT-"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import numpy as np\n","\n","def train_model(model, X_train, X_test, y_train, y_test):\n","    \"\"\"Train a model and evaluate it, returning predictions, accuracy, and other metrics.\"\"\"\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return model, y_pred, y_test\n","\n","def eval_classification_model(model, y_pred, y_test):\n","    accuracy = accuracy_score(y_test, y_pred)\n","    report = classification_report(y_test, y_pred, zero_division=1)\n","    conf_mat = confusion_matrix(y_test, y_pred)\n","    return accuracy, report, conf_mat\n","\n","def print_model_results(name, accuracy, report, conf_mat):\n","    \"\"\"Print the results of a model evaluation.\"\"\"\n","\n","    print(f\"Accuracy for {name} : {accuracy:.5f}\")\n","    print(\"Classification Report:\")\n","    print(report)\n","    print(\"Confusion Matrix:\")\n","    print(conf_mat)\n","    print('-' * 50)\n","\n","\n","def compare_models(X_train, X_test, y_train, y_test):\n","    \"\"\"Compare Logistic Regression, SVM, kNN, and Decision Tree models and print the best.\"\"\"\n","\n","    # Initialize models with best parameters for each (based on prior testing)\n","    models = {\n","        'Logistic Regression': LogisticRegression(),\n","        'Support Vector Machine': SVC(kernel='linear'),\n","        'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=2),\n","        'Decision Tree': DecisionTreeClassifier(max_depth=3)\n","    }\n","\n","    results = []\n","    best_model = None\n","    best_accuracy = 0\n","    accuracy_dict = {}\n","\n","    # Train and evaluate each model\n","    for name, model in models.items():\n","        model_, y_pred, y_test = train_model(model, X_train, X_test, y_train, y_test)\n","        accuracy, report, conf_mat = eval_classification_model(model_, y_pred, y_test)\n","\n","        results.append({'name': name, 'accuracy': accuracy, 'report': report, 'conf_mat': conf_mat})\n","        accuracy_dict[name] = accuracy\n","\n","        # Print results for each model\n","        print_model_results(name, accuracy, report, conf_mat)\n","\n","        # Update best model if current model has better accuracy\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_model = name\n","            best_conf = conf_mat\n","\n","    # Check if all models have the same accuracy (within a small tolerance)\n","    accuracies = np.array(list(accuracy_dict.values()))\n","    if np.allclose(accuracies, accuracies[0], atol=1e-5):  # handles floating point variations\n","        print(f\"All models have the same accuracy: {accuracies[0]:.5f}\")\n","        # print(f\"\\nWith confusion matrix: \\n {results[-1]['conf_mat']}\")\n","        # # tn, fp, fn, tp = results[-1]['conf_mat'].ravel()\n","        # # print(tn, fp, fn, tp)\n","\n","\n","\n","\n","    else:\n","        print(f\"\\nThe best model is: {best_model} with an accuracy of {best_accuracy:.5f}\")\n","        # print(f\"\\nWith confusion matrix: \\n {best_conf:.5f}\")\n","        print(best_conf)\n","\n","\n","    # Return the results for further analysis if needed\n","    return accuracy_dict\n","\n","# Example usage:\n","# Assuming X_train, X_test, y_train, y_test are defined\n","r = compare_models(X_train, X_test, y_train, y_test)\n","print(r)\n","\n","# In this case, the confusion matrix gives the following values:\n","# True Positives (TP) = 165\n","# False Negatives (FN) = 0\n","# False Positives (FP) = 28\n","# True Negatives (TN) = 0\n","\n","# So for the metrics we learned in class:\n","# Accuracy = (TP + TN) / (TP + TN + FP + FN) = (165 + 0) / (165 + 0 + 28 + 0) = 165 / 193 ≈ 0.8549 or 85.49%\n","# Precision = TP / (TP + FP) = 165 / (165 + 28) = 165 / 193 ≈ 0.8549 or 85.49%\n","# Recall (Sensitivity) = TP / (TP + FN) = 165 / (165 + 0) = 1 or 100%\n","# Specificity = TN / (TN + FP)= 0 / (0 + 28) = 0 or 0%\n","# F1 Score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.8549 * 1) / (0.8549 + 1) ≈ 0.9217 or 92.17%\n","\n","# This model is predicting all instances as positive. It has perfect recall for the positive class but zero specificity for the negative class.\n","# The accuracy and precision are the same because FN = TN = 0.\n","# The model's performance suggests it might be biased or the dataset might be imbalanced.\n","# It's correctly identifying all positive cases but failing to identify any negative cases, which could be problematic depending on the context of the classification task.\n","\n","\n","# when understanding this, i can realize that i need to manipulate the dataset.\n","# attached to the file a function for balancing the dataset.\n","# for undersampling, the the best model is Logistic Regression with 0.54 accuracy [[10 10] [11 15]]\n","# for oversampling, the best model is KNN with 0.9205 accuracy [[158 24] [3 155]]\n","\n","# i would choose oversampling rather than undersampling because it have better accuracy, yet i am risking the case of overfit."]},{"cell_type":"markdown","metadata":{"id":"6EbTP6PDboDw"},"source":[]},{"cell_type":"markdown","metadata":{"id":"aosUQvNzkgaT"},"source":["# Q1.d\n","Predict BMI based on age and Blood sodium with linear regression, SVM regressor, Decision tree regressor, and kNN refressor. Calculate RMSE, MSE, R-squared.\n","Split where 20% left for the test, random state=42."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPdX_TpCjjDD"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, r2_score\n","\n","models = {'Linear Regessor': LinearRegression(),\n","          'SVM Regressor': SVR(),\n","          'Decision Tree Regressor': DecisionTreeRegressor(),\n","          'KNN Regressor': KNeighborsRegressor()}\n","\n","X = pd.DataFrame(data, columns=['age','Blood sodium'])\n","y = pd.Series(data['BMI'], name='BMI') #choose one coloum\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","def eval_regression_model(name, y_pred, y_test):\n","    mse = mean_squared_error(y_test, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_test, y_pred)\n","    return (f\"{name} \\n MSE: {mse:.2f}, RMSE: {rmse:.2f}, R-squared: {r2:.2f}\")\n","\n","    # print(f\"{name} \\n MSE: {mse:.2f}, RMSE: {rmse:.2f}, R-squared: {r2:.2f} \\n ******* \\n\\n\\n\")\n","\n","    # return mse, r2\n","\n","def plot_pred(x,y,name):\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(x,y)\n","    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4) # approx the slope\n","    plt.title(f\"{name} \")\n","    plt.xlabel('Actual')\n","    plt.ylabel('Predicted')\n","\n","\n","def print_plot_models_results(models,X_train, X_test, y_train, y_test):\n","    for name, model in models.items():\n","        model, y_pred, y_test = train_model(models['Linear Regessor'],X_train, X_test, y_train, y_test)\n","        title = eval_regression_model(name, y_pred, y_test)\n","        print(title)\n","        plot_pred(y_pred,y_test,title)\n","\n","print_plot_models_results(models,X_train, X_test, y_train, y_test)\n","\n","# models_result = {}\n","# d['']\n","# # to do: make them all in a nice table and print that table\n","# def calc_stats(dat):\n","#   return dat.mean(),dat.mode()[0], dat.median(), dat.std()\n","\n","# #make it to a df\n","# d = {}\n","# d['age'] = calc_stats(data['age'])\n","# d['BMI'] = calc_stats(data['BMI'])\n","# d['Blood sodium'] = calc_stats(data['Blood sodium'])\n","\n","# d_ = pd.DataFrame(d)\n","# d_['stat']= ['mean','mode','median','std']\n","\n","# d_ = d_.set_index('stat')\n","# print(d_)\n"]},{"cell_type":"markdown","metadata":{"id":"NKZhwe7MxCze"},"source":["# Q2.a\n","Demonstrate the application of Principal Component Analysis (PCA) and t-Distributed\n","Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction on the dataset\n","\n","focusing on BMI, Blood sodium, and Blood calcium to visualize the data in a reduced-\n","dimensional space. Compare the visualization results of PCA and t-SNE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLWyfo-VyirK"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import seaborn as sns\n","\n","\n","# Prepare the data for the task\n","# remove rows with missing values for the releant features\n","data_clean = data.dropna(subset=['BMI', 'Blood sodium', 'Blood calcium'])\n","features = data_clean[['BMI', 'Blood sodium', 'Blood calcium']]\n","target_feature = data_clean['outcome']\n","\n","# calculate PCA\n","pca = PCA(n_components=2)\n","features_pca = pca.fit_transform(features)\n","\n","# calculate t-SNE\n","method = StandardScaler()\n","features_tSNE = method.fit_transform(features)\n","\n","# Plot both methods side by side\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","# Plot PCA\n","sns.scatterplot(\n","    x=features_pca[:, 0], y=features_pca[:, 1], hue=target_feature, palette='viridis', ax=ax1\n",")\n","ax1.set_title('PCA of BMI, Blood Sodium, Blood Calcium')\n","ax1.set_xlabel('PCA Component 1')\n","ax1.set_ylabel('PCA Component 2')\n","\n","# Plot t-SNE\n","sns.scatterplot(\n","    x=features_tSNE[:, 0], y=features_tSNE[:, 1], hue=target_feature, palette='viridis', ax=ax2\n","\n",")\n","\n","\n","ax2.set_title('t-SNE of BMI, Blood Sodium, Blood Calcium')\n","ax2.set_xlabel('t-SNE Dimension 1')\n","ax2.set_ylabel('t-SNE Dimension 2')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n","\n","# the outcome color is the original prediction.\n","# here we can see that the analysis didnt give us an evaluation n the prediction and also that the data is seems strongly correlated\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cmQU4_kNxnf8"},"source":["# Q2.b\n","Apply K-means clustering to the dataset to group patients based on age, BMI, diabetes, and heart rate. Cluster to 2,3,4,5, and 6 groups. What are Silhouette and Davies-Bouldin Score for each case?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-pyacDIbfT_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","import numpy as np\n","\n","# relevant data\n","data_clean = data[['age', 'BMI', 'diabetes', 'heart rate']].dropna()\n","\n","\n","# Number of clusters to try\n","cluster_list = list(range(2,7)) # relevant groups\n","silhouette_scores = []\n","davies_bouldin_scores = []\n","\n","# Figure setup for visualization\n","plt.figure(figsize=(15, 9))\n","\n","i=0\n","for cluster in cluster_list:\n","    # apply K-means clustering\n","    kmeans = KMeans(n_clusters=cluster, random_state=42)\n","    clusters = kmeans.fit_predict(data_clean)\n","\n","    # Silhouette score\n","    silhouette = silhouette_score(data_clean, clusters)\n","    silhouette_scores.append(silhouette)\n","\n","    # Davies-Bouldin score\n","    davies_bouldin = davies_bouldin_score(data_clean, clusters)\n","    davies_bouldin_scores.append(davies_bouldin)\n","    i+=1\n","\n","\n","# Print Silhouette and Davies-Bouldin Score for each case\n","davies_silhouette = {}\n","davies_silhouette['Davies Bouldin Score'] = davies_bouldin_scores\n","davies_silhouette['Silhouette Score'] = silhouette_scores\n","\n","davies_silhouette_ = pd.DataFrame(davies_silhouette)\n","davies_silhouette_['i'] = cluster_list\n","\n","davies_silhouette_=davies_silhouette_.set_index('i')\n","print(davies_silhouette_)\n"]},{"cell_type":"markdown","metadata":{"id":"kTno5l_f4jpz"},"source":["#Q3.a\n","Describe the steps involved in training a neural network, including forward propagation and backpropagation."]},{"cell_type":"markdown","metadata":{"id":"8hTEcrfRuRLP"},"source":["## Answer\n","Steps involved in training a neural network:\n","\n","1. Initialization: Set random initial weights and biases.\n","2. Forward propagation:\n","    a. Input data flows through the network\n","    b. Each neuron computes weighted sum of inputs and applies activation function\n","    c. Output is produced at the final layer\n","3. Loss calculation: Compare network output to expected output using a loss function\n","4. Backpropagation:\n","    a. Calculate gradients of the loss with respect to weights and biases\n","    b. Propagate these gradients backwards through the network\n","5. Parameter update: Adjust weights and biases using an optimization algorithm (eg., gradient descent)\n","6. Repeat steps 2-5 for multiple epochs until convergence or stopping criterion is met (the error/cost)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IlrgV_qU4vpd"},"source":["# Q3.b\n","Explain the bias-variance trade-off in neural network performance. How does it affect model generalization?"]},{"cell_type":"markdown","metadata":{"id":"XIgQibOntV5G"},"source":["## Answer\n","The bias-variance trade-off refers to the balance between a model's ability to fit the training data (low bias) and its ability to generalize to new, unseen data (low variance).\n","\n","    High bias: Model is too simple, underfits the data\n","    High variance: Model is too complex, overfits the data\n","\n","This trade-off affects generalization in the following ways:\n","\n","    A model with high bias may fail to capture important patterns in the data\n","    A model with high variance may fit noise in the training data, leading to poor performance on new data\n","\n","The goal is to find an optimal balance that minimizes both bias and variance, resulting in a model that generalizes well to new data.\n"]},{"cell_type":"markdown","metadata":{"id":"w2wMvtEj5Mpp"},"source":["# Q3.c\n","Highlight the importance of data preprocessing, normalization, and splitting for training effective deep learning models."]},{"cell_type":"markdown","metadata":{"id":"nnRJf3yKvjb3"},"source":["## Answer\n","\n","1. Data preprocessing:\n","    Removes noise and irrelevant information\n","    Handles missing values and outliers\n","    Improves model performance and training efficiency\n","\n","2. Normalization:\n","    Scales features to a common range (e.g., 0-1 or -1 to 1)\n","    Ensures all features contribute equally to the model\n","    Helps with faster convergence during training\n","    \n","3. Data splitting:\n","    Divides data into training, validation, and test sets\n","    Training set: Used to train the model\n","    Validation set: Used for hyperparameter tuning and model selection\n","    Test set: Used to evaluate final model performance on unseen data\n","    Helps assess model generalization and prevent overfitting\n","\n","These processes are crucial for developing effective deep learning models by ensuring the data is clean, properly scaled, and appropriately divided for training and evaluation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uuTr0OVu5XJd"},"source":["# Q3.d+e"]},{"cell_type":"markdown","metadata":{"id":"WJ-QZ9077KX4"},"source":["Relevant library from tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rF2BHVgj69RF"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{"id":"le4yhraX7P7P"},"source":["Ensuring data is clean and ready with all features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPOT5kcl7Gz_"},"outputs":[],"source":["# 1\n","\n","# clean the data and select relevant features\n","clean_data = data[['age','Blood sodium','outcome']].dropna()\n","X = clean_data[['age','Blood sodium']]\n","y = clean_data[['outcome']]\n","\n","# standardtize the data\n","X_standrd = StandardScaler()\n","X_standrd = X_standrd.fit_transform(X)\n","\n","\n","# 2\n","X_train, X_test, y_train, y_test = train_test_split(X_standrd, y, test_size=0.2, random_state=42)\n","\n","# 3 construct DNN\n","model = Sequential([\n","    Dense(3, activation='relu',input_shape=(X_train.shape[1],)),\n","    Dropout(0.05),\n","    Dense(3, activation='relu'),\n","    Dropout(0.05),\n","    Dense(3, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# 4 Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n","eval_ = model.evaluate(X_test, y_test, verbose=1)[1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9DcVbvcbfUC"},"outputs":[],"source":["# Plotting functions\n","def plot_accuracy(history):\n","    plt.subplot(1,2,1)\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","\n","    # accuracy = model.evaluate(X_test, y_test, verbose=1)[1]\n","\n","    plt.title('Model Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    return plt\n","\n","def plot_loss(history):\n","    plt.subplot(1,2,2)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    return plt\n","\n","# Plot accuracy and loss\n","plt.figure(figsize=(10, 6))\n","plot_accuracy(history)\n","plot_loss(history)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCRBQbZXbfUC"},"outputs":[],"source":["# 5 - Evaluate and Visualize Model Performance\n","y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n","conf_mat = confusion_matrix(y_test, y_pred)\n","# Plot the confusion matrix using seaborn\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n","\n","# Set labels and title\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","\n","# Show the plot\n","plt.show()\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"ZuTUHxSEbfUC"},"source":["# My own network!\n","This part was done using W&B, which helped my keep track of the accuracy values.\n","From the over-sampled data set, i preformed a fit for each feature alone,\n","I took the feature's whos accuracy was more than 0.5 (meaning more than by chance), and used them to train a new model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"om-BRXm3bfUC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import wandb\n","from wandb.integration.keras import WandbCallback\n","\n","# Load the data\n","# wandb.login()\n","\n","\n","# build input pipeline using tf.data\n","\n","\n","def get_train_and_val(data_feaure_and_prediction, BATCH_SIZE = 64):\n","    \"\"\"_summary_\n","    this function gets the relevant feaure and variables\n","    Args:\n","        data_feaure_and_prediction (_type_): the last colum should be the output, the first ones should be the feature(s)\n","        BATCH_SIZE (int, optional): _description_. Defaults to 64.\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    # data_feaure_and_prediction = data_feaure_and_prediction.dropna() # drop the irrelevant values\n","    X = data_feaure_and_prediction[data_feaure_and_prediction.keys()[:-1]] # the feature\n","    y = data_feaure_and_prediction[data_feaure_and_prediction.keys()[-1]] # its prediction\n","\n","    X_standard = StandardScaler()\n","    X_standard = X_standard.fit_transform(X)\n","    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2, random_state=42)\n","\n","    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","    val_dataset = val_dataset.batch(BATCH_SIZE)\n","    return [train_dataset, val_dataset]\n","\n","def make_model(feature_num, input_name=\"input name\"):\n","    inputs = keras.Input((feature_num,), name=input_name)\n","    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n","    x1 = keras.layers.Dropout(0.3)(x1)\n","\n","    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n","    x2 = keras.layers.Dropout(0.3)(x2)\n","\n","    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n","\n","    return keras.Model(inputs=inputs, outputs=outputs)\n","\n","def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","    train_acc_metric.update_state(y, logits) # update the weights of the network\n","\n","    return loss_value\n","\n","def test_step(x, y, model, loss_fn, val_acc_metric):\n","    val_logits = model(x, training=False)\n","    loss_value = loss_fn(y, val_logits)\n","    val_acc_metric.update_state(y, val_logits)\n","\n","    return loss_value\n","\n","def train(train_dataset, val_dataset,  model, optimizer,\n","          train_acc_metric, val_acc_metric, loss_fn,\n","          epochs=10, log_step=200, val_log_step=50):\n","\n","    for epoch in range(epochs):\n","        print(\"\\nStart of epoch %d\" % (epoch,))\n","\n","        train_loss = []\n","        val_loss = []\n","\n","        # Iterate over the batches of the dataset\n","        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","            loss_value = train_step(x_batch_train, y_batch_train,\n","                                    model, optimizer,\n","                                    loss_fn, train_acc_metric)\n","            train_loss.append(float(loss_value))\n","\n","        # Run a validation loop at the end of each epoch\n","        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n","            val_loss_value = test_step(x_batch_val, y_batch_val,\n","                                       model, loss_fn,\n","                                       val_acc_metric)\n","            val_loss.append(float(val_loss_value))\n","\n","        # Display metrics at the end of each epoch\n","        train_acc = train_acc_metric.result()\n","        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","        val_acc = val_acc_metric.result()\n","        print(\"Validation acc: %.4f\" % (float(val_acc),))\n","\n","        # Reset metrics at the end of each epoch\n","        train_acc_metric.reset_state()\n","        val_acc_metric.reset_state()\n","\n","        # ⭐: log metrics using wandb.log\n","        wandb.log({'epochs': epoch,\n","                   'loss': np.mean(train_loss),\n","                   'acc': float(train_acc),\n","                   'val_loss': np.mean(val_loss),\n","                   'val_acc':float(val_acc)})\n","\n","# initialize wandb with your project name and optionally with configutations.\n","# play around with the config values and see the result on your wandb dashboard.\n","\n","def run_with_feature(feature):\n","      config = {\n","            \"epochs\": 20,\n","            \"batch_size\": 32,\n","            \"log_step\": 200,\n","            \"val_log_step\": 50,\n","            \"feature_used\": str(feature)\n","      }\n","\n","      run = wandb.init(project='my-tf-integration', config=config)\n","      config = wandb.config\n","\n","      prediction = \"outcome\"\n","\n","      model_data = data[feature+[prediction]].dropna()\n","      [train_dataset, val_dataset] = get_train_and_val(model_data)\n","\n","\n","      # Initialize model.\n","      model = make_model(len(feature))\n","\n","      # Instantiate an optimizer to train the model.\n","      optimizer = keras.optimizers.Adam()\n","      # Instantiate a loss function.\n","      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","      # Prepare the metrics.\n","      train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","      val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","\n","\n","      train(train_dataset,\n","            val_dataset,\n","            model,\n","            optimizer,\n","            train_acc_metric,\n","            val_acc_metric,\n","            loss_fn,\n","            epochs=config.epochs,\n","            log_step=config.log_step,\n","            val_log_step=config.val_log_step)\n","\n","      run.finish()  # In Jupyter/Colab, let us know you're finished!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhPi8vx-bfUD"},"outputs":[],"source":["# Determine the features to train the ulimate model!\n","data_keys = data.drop(columns=['outcome']).keys()\n","len(data_keys)\n","# data_keys = data_keys[\n","\n","train_acc_dict = dict.fromkeys(data_keys, 0)\n","val_acc_dict = dict.fromkeys(data_keys, 0)\n","\n","for elem in data_keys:\n","    # try:\n","    run_with_feature([elem])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WPGPdY7bfUD"},"outputs":[],"source":["# The features with accuracy score over 0.5 from the last block, used to train the model\n","\n","feature1 =[\"Urine output\", \"Lymphocyte\", \"Bicarbonate\", \"Leucocyte\", \"Urea nitrogen\",\n","    \"Anion gap\", \"Neutrophils\", \"Blood calcium\", \"Lactic acid\", \"PH\", \"Basophils\",\n","    \"Respiratory rate\", \"Blood sodium\", \"RDW\", \"Blood potassium\", \"NT-proBNP\", \"PT\",\n","    \"Systolic blood pressure\", \"heart rate\", \"Renal failure\", \"Chloride\", \"Platelets\",\n","    \"INR\", \"atrialfibrillation\", \"deficiencyanemias\", \"Diastolic blood pressure\",\n","    \"hypertensive\", \"EF\", \"BMI\", \"Magnesium ion\", \"temperature\", \"Creatinine\", \"MCH\",\n","    \"PCO2\", \"SP O2\", \"hematocrit\", \"MCV\", \"diabetes\", \"Hyperlipemia\", \"age\", \"ID\",\n","    \"COPD\", \"CHD with no MI\", \"Creatine kinase\", \"glucose\", \"gendera\", \"depression\"\n","]\n","\n","run_with_feature(feature1)\n"]},{"cell_type":"markdown","metadata":{"id":"_gRKrEtibfUJ"},"source":["# The results I got for this model\n","Run history:\n","\n","acc\t▁▄▆▆▆▆▇▇▇▇▇▇█▇██████\n","\n","epochs\t▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\n","loss\t█▅▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n","\n","val_acc\t▁▄▅▆▆▆▇▇▇▇██████████\n","\n","val_loss\t█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n","\n","\n","Run summary:\n","\n","acc\t0.90554\n","\n","epochs\t19\n","\n","loss\t0.24534\n","\n","val_acc\t0.9026\n","\n","val_loss\t0.25483"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
